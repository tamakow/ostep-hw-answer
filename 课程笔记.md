## 多处理器编程：从入门到放弃
笔记在chap 26-27

如果在c程序#include使用的是<>，那么会从系统include路径搜索

#### threads.h
threads.h的主要数据结构
```c
struct thread {
  int id;               // 从 1 开始的线程号
  pthread_t thread;     // pthreads 线程号
  void (*entry)(int);   // 线程的入口函数
  struct thread *next;  // 链表
};
```
entry是函数指针，指向一个 void fn(int)函数

threads.h中封装的一些API
```c
void create(void *fn);   // 创建一个线程立即执行，入口代码为 void fn(int tid);
void join(void (*fn)()); // 等待所有线程执行结束后，调用 fn
```

ppt第九页为什么要
```c
setbuf(stdout, NULL);
```
stdout是行缓冲的，只有当遇到换行符或者缓存区满的时候才会输出，这里把stdout的缓冲设置为0，使得其立即输出


线程共享代码数据，但拥有独立的堆栈，这导致了致命的问题
1. 原子性丧失（代码执行会被其他线程干扰，原子性被破坏）
2. 顺序丧失（语句的执行并不严格按照编写的顺序发生。编译器对程序的改写（调整内存访问指令的顺序）对于多线程程序不再保证正确）
3. 可见性丧失（CPU可以不按顺序执行指令，指令的结果不能立即对其他处理器可见）

#### 一定程度保证顺序
* volatile：防止优化编译器把变量从内存装入CPU寄存器中，即“直接存取原始内存地址”

```c
volatile char* ptr; // 指针指向的对象是volatile的
char* volatile ptr; //指针自身的值（即一个代表地址的值）是volatile的
```
这和const的常量指针和指针常量是类似的。
#### 保护原子性
互斥

#### 思考题
1. 如何证明真的启动了多个线程？
   * 创立两个线程，每个都永远输出两个不同的字符，然后观察结果
2. 如何证明共享内存？
   * 不同线程对全局变量的修改
3. 如何知道每个线程的堆栈范围和大小
   * 不断递归直到crash，同时-输出堆栈点
4. 26.2的问题
   1. 开O1优化结果为n：O1优化将不会再每次循环更新sum，而会循环n次之后只写入sum一次（在循环开始前将sum初值取到寄存器里，循环n次结束后再给回sum，由于循环时间比较长，所以初值基本都是取到0，最后给回sum的值都为n）
   2. 开O2结果为2n：直接优化成addq （一次给sum加n）

#### atexit
The atexit() function registers the given function to be called at normal process termination
也就是说，atexit是在正常的进程终止后才会被调用的，如果在main函数中发生了非法操作（如执行非法的指针解引用），那么atexit注册的callback不会被运行
测试代码为

```c
#include <stdio.h>
#include <stdlib.h>

void bye() {
    fprintf(stderr, "That's all\n");
}

int main() {
    int i;
    i = atexit(bye);
    fprintf(stderr, "main\n");
    *(int *)NULL = 0;
    if(i != 0) {
        fprintf(stderr, "can't set exit function!\n");
        exit(EXIT_FAILURE);
    }
    exit(EXIT_SUCCESS);
}
```


## 阅读理解并发程序
从状态机视角看算法的运行

软件实现互斥：
Peterson 算法：
```c
int turn, x = 0, y = 0;

void thread1() {
  [1] x = 1;
  [2] turn = T2;
  [3] while (y && turn == T2) ;
  [4] // critical section
  [5] x = 0;
}

void thread2() {
  [1] y = 1;
  [2] turn = T1;
  [3] while (x && turn == T1) ;
  [4] // critical section
  [5] y = 0;
}
```
缺点：
1. 只能用于两个线程的互斥
   * 多线程的扩展：filter lock；性能低
2. 小心编译器和多处理器硬件
   * 性能低
3. 正确性是有前提的：运行在理想的计算机上，即认为计算机执行指令具有原子性、顺序和可见性
   1. 但实际上，指令执行也不是原子不可分割的
   2. 在现代系统上基本上不可用

## 并发控制：互斥（1）
chapter 28-29
软件并不够实现互斥，那么使用硬件来提供吧  
硬件提供的机制还给了我们救回多处理器共享内存程序上的原子性、顺序和可见性的期望。

我们希望lock保护的临界区是一个不可分割的盒子。阻止盒子并发就意味着如果考虑状态机上的所有路径，盒子的执行能排出一个顺序（而不是并发）
* 原子性： 两个盒子一定有前后顺序
* 顺序： 盒子中对共享内存的乱序读写不能越过lock/unlock的边界。这就限制了实际处理器执行指令时的乱序行为只能在盒子 (同一个线程) 中发生。
* 可见性：如果两个盒子按照$B_1\rightarrow B_2$的顺序执行，那么$B_1$中所有的写操作，都必须在$B_2$中可见，反之亦然。这保证了盒子之间共享内存访问的可见性。

xchg原子性完成读写  
xchg实现的自旋锁
```c
int locked;

void initlock() {
   locked = 0;
}

void lock() {
  while (xchg(&locked, 1)) ;
}

void unlock() {
  xchg(&locked, 0);
}
```

#### 数据竞争
不同的线程同时访问同一段内存，且至少有一个是写

**用互斥锁保护好共享数据，消灭一切数据竞争**

详细讲了L1的设计思路

## 并发控制：互斥（2）
使用操作系统和硬件机制实现非自旋的锁
* yield锁
* 互斥锁
* futex：自旋+等待

用户程序的互斥（futex）
操作系统内核中的互斥（关中断+自旋）


一个死机的原因：
* 关中断的时候死循环

自旋锁的浪费
* 多个处理器上的线程相互竞争，一个获得锁，其他CPU上的进程只能围观（进行无用的运算浪费性能）
* 操作系统会中断，获得锁的进程完成时间更晚，导致cpu利用率进一步下降

#### yield 锁
如何改进自旋锁的性能
* 等待锁的线程
  * 让他不占用处理器自旋，如果系统中还有不需要这把锁的线程，可能让他们先执行

一个trivial的实现
```c
void yield_lock(spinlock_t *lk) {
  while (xchg(&lk->locked, 1)) {
    syscall(SYS_yield); // yield() on AbstractMachine
  }
}
void yield_unlock(spinlock_t *lk) {
  xchg(&lk->locked, 0);
}
```

#### 互斥锁
上面那个版本如果等待锁的线程非常非常多，我们需要 “空转一轮” 才能让一个真正需要运行的线程执行。实际上，系统调用已经实现了更优化的版本（暂停需要锁的进程，等解锁后再调度执行）
```c
mutex_lock(&locked) //如果 xchg(&locked, 1) == 1，就不再调度当前线程，即为当前线程标记上 “block on locked”
mutex_unlock(&locked) //xchg(&locked, 0); 如果有 “block on locked” 的线程，修改其状态为可调度（由操作系统控制）
```

#### Futex
自旋锁 (线程直接共享 locked)
* 好处：更快的fast path
  * xchg成功：立即进入临界区，开销很小
* 坏处：更慢的 slow path
  * xchg 失败：浪费CPU自旋等待

睡眠锁 (locked 保存在操作系统内核，通过系统调用访问)
* 好处：更快的 slow path
  * 上锁失败线程不再占用 CPU
* 坏处：更慢的 fast path
  * 即便上锁成功也需要进出内核 (syscall)

结合两者的好处！
* fast path: 一条原子指令，上锁成功立即返回
* slow path: 上锁失败，执行系统调用睡眠

这就是pthreads库中的互斥锁(pthread_mutex)

Futex: Fast Userspace muTexes
```c
void futex_lock() {
  while (1) {
    int r = xchg(&locked, 1);
    if (r == 0) break;
    syscall(SYS_futex, &locked, FUTEX_WAIT, 1);
  }
}

void futex_unlock() {
  xchg(&locked, 0);
  syscall(SYS_futex, &locked, FUTEX_WAKE, 1);
}
```
但这可以改进
* unlock还有syscall


## 并发控制：同步
见chap30-31

条件变量的小缺陷
* broadcast会唤醒不必要的线程从而引起一些性能上的浪费

所以引入了信号量  
信号量 = 互斥锁 + 条件变量 + 计数器
* 互斥锁
  * 仅有一个手环
* 条件变量
  * 手环 = 等待条件

## 并发编程模型


## 虚拟化：进程抽象
用“状态机”的视角重新理解进程
* 学习UNIX进程管理API

#### fork()

做一份状态机完整的复制（内存、寄存器现场）
* 新创建的进程返回 0，执行 fork 的进程返回进程号

如何理解fork程序？  
画状态机！

**问题**
多线程程序的某个线程执行fork，应该会发生什么？  
没有办法复制所有的进程，因为不知道其他线程正在做什么。所以现在的fork只能复制一个线程，其他线程就不管了


#### printf的离谱之处
我们都知道在printf输出到终端的时候，会有行缓冲，即在缓冲区满了或者遇到换行符才会输出（为了节省系统调用）  
但是，printf的输出作为管道输出到文件里时，libc会认为是大量数据的写入，所以即使遇到换行符也不会输出，要等到缓冲区满了或者数据读入结束才会输出到文件里

man setbuf后知道有三种缓冲方式
* 无缓冲：立即写。如stderr
* 块缓冲：先存储块大小的字符再输出（或者写入结束）
* 行缓冲：遇到换行符再输出（或者缓冲区满了）

一般来说，文件都是块缓冲的，而终端(stdout)是行缓冲的，这就解释了printf的行为

#### execve()
将当前运行的状态机重置成另一个程序的初始状态
```c
int execve(const char *filename, char * const argv, char * const envp);
```
* 执行名为 filename 的程序
* 允许对新状态机设置参数 argv (v) 和环境变量 envp (e)
  * 对应着main的参数

**环境变量**：程序执行的环境
  * PATH： 可执行文件搜索路径（execve会一个一个在path中找）


#### _exit()
立即摧毁状态机

void _exit(int status)
* 销毁当前状态机，并允许有一个返回值
* 子进程终止会通知父进程

atexit只有在库函数exit()时callback才会执行，在_exit()和__exit()时不会执行callback

_exit():使用exit_group摧毁状态机，销毁所有线程
__exit()：直接使用exit系统调用，只销毁一个线程，其他线程保留，相当于syscall(SYS_exit, 0);

#### shell执行指令的流程
shell是一个工具人用户程序，给你展示一个提示符，等待你向它输入指令。shell接着找出你要来执行的程序在哪里，然后调用fork()来创建一个子进程，再调用exec()的各种变种来执行程序。最后，shell通过wait()等待程序完成退出，此时shell从系统调用中返回，打印新的提示，等待用户的下一个输入。


## Fork杂谈
在之前，我们认为的状态机模型是(M,R)，即内存和寄存器，这个模型其实有一点oversimplified了，因为进程还要访问操作系统中的对象（例如write，如何确定写入到哪？stdout? stderr?）

操作系统中的对象都是文件，所以进程访问操作系统对象时只需要**文件描述符**
* 文件描述符是一个指向操作系统对象的指针
* open 返回文件描述符
* read， write改变文件描述符

常见的文件描述符
* 0 - stdin
* 1 - stdout
* 2 - stderr
* 其他由open创建（总是返回最小的可用编号）

/proc/[pid]/fd：进程打开的所有文件

#### fork的新认知
fork()时进程的所有文件描述符也会被子进程继承

后设计的API：spawn

Fork的七宗罪
见slides

如果使用了fork跳过初始化，那么之后的地址空间都是确定的了（可以参考slides中的nemu初始化，一份初始化，之后每次start都是fork创建进程），而正常的加载会使得地址空间是随机的。


## 虚拟化：内存的抽象
BSS（Block Started by Symbol）通常是指用来存放程序中未初始化的全局变量和静态变量的一块内存区域。特点是:可读写的，在程序执行之前BSS段会自动清0。所以，未初始的全局变量在程序执行之前已经成0了。
data段存放的是初始化后的全局变量和静态变量。

如何知道pmap是怎么实现的？  
strace

pmap显示内存映像。由上到下就是：ELF文件的代码段、各共享库的数据段、用户代码段、用户数据段、堆、栈

只读可执行是代码段，可读可写但没有执行权限说明是数据段


pmap最后三个[vvar],[vdso],[vsyscall]是什么？  
vdso 和 vsyscall 的作用是都是加速系统调用
* vsyscall：可读的系统调用可能可以不用陷入内核执行，静态链接，容易引发安全问题
* vdso：vitual dynamically linked shared objects，由于是动态链接，安全性更高。当不支持vdso时，使用vsyscall
* vvar:内核和进程共享的数据(只读)

OS在把程序加载到内存时，其实做的事情很少
* 静态链接：代码、数据、堆栈、堆区
* 动态链接： 代码、数据、堆栈、堆区、INTERP (ld.so)

那么像libc.so那些是怎么加载进来的？用了什么系统调用？  
**mmap**

#### mmap
```c
// 映射
void *mmap(void *addr, size_t length, int prot, int flags, int fd, off_t offset);
void *munmap(void *addr, size_t length);
```
在状态机状态M上增加/删除一段可访问（读/写/执行）的内存  

* addr - 地址空间中的地址（如果是NULL的话由os决定，如果地址上已经由有映射了，换个地方，也是os决定）
* length - 长度
* prot - 权限（可读/可写/可执行）
* flags - 指定映射的项的类型，映射选型和映射页是否可以共享
* fd： 有效的文件描述符
* offset：被映射对象内容的起点


```c
// 修改映射权限
int mprotect(void *addr, size_t length, int prot);
```

/proc/[pid]/mem：进程的内存信息

gdb：会读写别的程序的内存（实际上是ptrace）

操作系统是如何使用mmap的？  
实际上啥都没做，他只是在地址空间里找到一个可以放的下的地方，然后把一些信息存储下来，实际上的数据一个都没加载。在之后访问的时候会发生缺页（cr2寄存器），那个时候再来加载一个页面


#### 虚拟存储的实现：分页
控制寄存器(CR0,CR1,CR2,CR3,CR4)
* CR0存储一些信息位
  * 0位是保护允许位PE(Protedted Enable)
* CR1是未定义的控制寄存器，供将来的处理器使用。
* CR2是页故障线性地址寄存器，保存最后一次出现页故障的全32位线性地址。
* CR3是页目录基址寄存器，保存页目录表的物理地址，页目录表总是放在以4K字节为单位的存储器边界上，因此，它的地址的低12位总为0，不起作用，即使写上内容，也不会被理会。
* CR4在Pentium系列（包括486的后期版本）处理器中才实现，它处理的事务包括诸如何时启用虚拟8086模式等

我们的需求是把“虚拟地址”翻译成“物理地址”
* 分页，映射即为一个页面到一个页面的映射
* Trie + TLB：类似cache，存储最有可能访问的页表信息。
* 分页还使得映射的低位可以存储一些保护信息



## 处理器调度（不考）
chap 7 - 11
期末考试不考，真是不幸中的万幸

### 虚假（课本上的）处理器调度
假设
* 一个处理器
* 多个进程共享CPU
  * 包括系统调用(一部分代码在syscall中执行)
  * 偶尔会等IO返回，不适用CPU（IO一般时间比较长）
* 处理器以固定的频率被中断
* 随时有新的进程被创建/旧的进程退出

中断
* 中断/系统调用中可以切换到其他线程执行

**Round-Robin**
假设当前$T_i$运行
* 中断后视图切换到下一个线程$T_{(i+1) \mod n}$
* 如果下一个线程正在等待I/O，继续尝试下一个
* 如果所有线程都不需要CPU，就调度idle进程执行

引入优先级

### 真正的处理器调度
**MLFQ**
在课本第8章
有一些不同的队列(queue)，每个都有不同的优先级(priority level)。在任意时刻，一个即将运行的任务只会在一个队列中。

基本规则：
* 规则1：如果A的优先级大于B的优先级，运行A；
* 规则2：如果A、B优先级相同，用RR调度它们。

MLFQ根据它观察到的进程行为来区分进程之间的优先级。例如，一个不断主动放弃CPU等待键盘输入的进程会拥有高优先级；而长时间使用CPU计算的进程的优先级会被降低。MLFQ会尝试学习运行的进程的历史，来决定它未来的行为。

**尝试1：如何改变优先级**
我们认为负载有两种：一种是运行时间短，需要交互（可能经常主动放弃CPU）的任务；另一种是长时间占用CPU，响应时间不重要的任务。对此，优先级调整算法如下：
* 规则3：当一个程序进入系统，默认优先级是最高的。
* 规则4a：如果程序使用了整段运行时间，降低优先级；
* 规则4b：如果程序在时间结束前主动放弃CPU，保持优先级不变。

但是，这样的MLFQ会带来一个问题：如果系统中有太多交互型的程序，他们的优先级高，会保持对CPU的占用，那么低优先级的程序就永远无法得到CPU时间(starve)。

其次，恶心人的用户会破坏调度算法的合理性(game the scheduler)，在时间截止前进程发起一个（不关心结果的）I/O请求来放弃CPU，就可以保持在同一个优先级队列中，获得更多的CPU时间。

最后，进程的行为可能随时间改变，一个计算型程序可能会变为交互型程序。此时这个程序无法重新获得优先级，响应时间高。

**尝试2：优先级提升**
* 规则5：经过一段时间$S$后，把系统中的所有任务移动到最高优先级队列中。

解决两个问题  
首先，低优先级的程序不会被高优先级的阻塞掉；其次，能够处理程序行为的变化。

但是如何确定$S$?

**尝试3：更好的统计运行时间**
规则4：一旦程序用完了对应优先级允许的运行时间（无论放弃了多少次CPU），都把它移动到次优先级。

这个解决了用户恶意利用调度规则的问题

#### Linux的调度策略：CFS
试图去模拟一个理想的多任务CPU  
* 为每个进程记录精确的运行时间
* 中断/异常发生后，切换到运行时间最少的进程执行
  
引入优先级 -20 ~ 19
* 越 nice， 越被不nice的人抢占

没时间了，不看了，等考完试再来补

## 持久化： 1-bit 信息的存储
* 文件是如何实现的？
* 为什么关机以后文件还在？

Persistence: “A firm or obstinate continuance in a course of action in spite of difficulty or opposition.”

科普性质的
* 磁
* 光
* 电

Flash Memory的缺点？
NAND Flash 有一个很严重的缺陷——擦除的次数有数千到上万的上限。这是因为每次 erase 操作时都会进行一次放电，但放电无法放到 100% 干净，到一定寿命之后，就始终是充电的状态。

## I/O设备和驱动
### 简单介绍
#### CPU
* 取指令
* 译码
* 执行

I/O 设备是一个能与 CPU 交换数据的接口(寄存器)
* 人话说就是“几组线”
  * 每一组线有约定的功能
  * CPU通过握手信号从线上读出、写入数据
* 每一组线有自己的地址
  * CPU 可以直接使用指令 (in/out/MMIO) 和设备交换数据

#### 键盘
两个I/O接口
* 0x60(DATA)
* 0x64(status/command)

#### 磁盘控制器
* IDE接口磁盘
  * primary:0x1f0-0x1f7
  * secondary: 0x170-0x177

#### 中断控制器
CPU有一个中断引脚
* 收到某个特定的电信号会触发中断
  * 保存5个寄存器(cs,rip,rflags,ss,rsp)
  * 跳转到中断向量表对应项执行

#### 总线
I/O设备越来越多，但是CPU引脚不能无限制增加  
总线：一个特殊的I/O设备  
* 提供地址到设备的转发
  * 把收到的地址 (总线地址) 和数据转发到相应的设备上

这样 CPU 只需要直连一个总线就可以了

#### DMA
假设程序希望写入 1 GB 的数据到磁盘  
即便磁盘已经准备好，依然需要非常浪费缓慢的循环

如何把CPU从执行循环中解放出来
  
**DMA**:一个专门执行 “memcpy” 程序的 CPU
支持的几种memcpy
* memory -> memory
* memory -> device(register)
* device(register) -> memory

直接把 DMA 控制器连接在总线和内存上

#### GPU(显卡)
用于 “加速图形显示” 的硬件  
GPU上程序最重要的特性就是并行

### I/O设备抽象
设备 = 支持以下三种操作的对象 (文件)
* read: 从设备某个指定的位置读出数据
* write: 向设备某个指定位置写入数据
* ioctl: 读取/设置设备的状态

设备驱动程序：将设备抽象为一个对象和操作

两个ioctl
```c
long (*unlocked_ioctl) (struct file *, unsigned int, unsigned long);
long (*compat_ioctl) (struct file *, unsigned int, unsigned long);
```

unlocked_ioctl: BKL (Big Kernel Lock) 时代的遗产
* 单处理器时代只有 ioctl
* 之后引入了 BKL, ioctl 执行时默认持有 BKL
* (2.6.11) 高性能的驱动可以通过 unlocked_ioctl 避免锁
* (2.6.36) ioctl 从 struct file_operations 中移除

compact_ioctl: 机器字长的兼容性
* 32-bit 程序在 64-bit 系统上可以 ioctl
* 此时应用程序和操作系统对 ioctl 数据结构的解读可能不同 (tty)
* (调用此兼容模式)


## 文件系统API
在 1-bit 的存储中，已经保证了各类数据等对象可以持久的保存在介质里

但让应用程序直接通过驱动访问存储设备 (1950s)？  
今天的系统中有不止一个程序，每个程序还需要考虑各种访问权限、并发控制。如果程序出问题了，不小心弄坏了整块磁盘？

文件系统的设计目标
* 提供合理的 API 使多个应用程序能共享数据
* 提供一定的隔离，使恶意/出错程序的伤害不能任意扩大

### 文件系统：存储设备的虚拟化
磁盘 (I/O 设备) = 一个可以读/写的字节序列  
虚拟磁盘 (文件) = 一个可以读/写/的动态字节序列

可以类比
* 进程抽象：一个 CPU -> 在时间上共享
* 虚拟存储：一份内存 -> 划分给多个虚拟地址空间
* 文件系统：一个物理磁盘 -> 多个虚拟磁盘

这里的虚拟磁盘其实就是文件和目录的集合
文件系统其实就是一个路径到虚拟磁盘的映射

文件系统 API: 虚拟磁盘管理  
需要解决的问题
* 虚拟磁盘的命名、查找、权限
* 虚拟磁盘的操作 (读写)


#### 虚拟磁盘：命名管理
文件系统就是“虚拟磁盘名”到“虚拟磁盘对象”的映射，即在存储设备上组织文件的方法

目录：文件/目录的集合 (形成一棵树)
* 逻辑相关的数据存放在相近的目录

文件系统的根
* windows
  * 一个驱动器就是一棵树
  * 从C盘开始，A,B盘是软盘
* unix/linux
  * 只有一个根 /

那么linux的第二个设备和U盘呢？  
在/dev下多了一个设备

**mount！**
可以把一个设备挂载到任何一个目录上，原目录下文件暂时消失（但不会清除）  
mount会把设备里的文件系统解析出来，然后再挂载到所给目录

mount 完成的工作是把一个设备和一个文件系统实现联系起来，在设备上创建一个文件系统实例，并且把创建的文件系统 “替换” 到文件系统中的一个目录中。

如何mount一个文件？  
文件只要符合规范，就可以生成磁盘镜像，系统会将镜像虚拟化成一个回环设备(ioctl)，然后再调用mount

在unix里文件和目录是完全不一样的东西  
link系统调用：(假设 ln a.txt b.txt)
创建文件a.txt的一个硬链接（即类似一个指向该文件的指针）b.txt  
这里b.txt是a.txt的一个别名，当a.txt改变的时候，所有link到a.txt的文件都会改变

硬链接：允许一个文件被多个目录引用
* 目录中仅存储指向文件数据的指针
* 不能链接目录
* 不能跨文件系统

一个文件的硬链接文件编号一致
如果文件A硬链接文件B，文件B硬链接文件C，文件C的本质时一个inode为10的文件，则A->(inode)10,B->(inode)10,C->(inode)10

软链接（符号链接）：相当于一个快捷方式（指向链接的那个文件，比如文件A链接文件B，文件B链接文件C，就有 A->B->C）
* 软链接也是一个文件
* 当引用这个文件时，去找另一个文件
* 另一个文件的绝对/相对路径以文本形式存储在文件里
* 可以跨文件系统、可以链接目录

一个文件的不同软连接文件编号是不一样的

unlink：删除一个硬链接

软链接的麻烦
* 软链接可能成环

文件的读写自带偏移量，这样就不用每次都指定文件读/写到哪里了  
一个问题：fork后父子进程同时写入文件，如果各自持有偏移量，就会有race condition。我们使用共享偏移量，操作系统来保证write的原子性

* open 时，获得一个独立的 offset
* dup 时，两个文件描述符共享 offset
* fork 时，父子进程共享 offset
* execve 时文件描述符不变
* O_APPEND 方式打开的文件，偏移量永远在最后 (无论是否 fork)


## 文件系统实现
在Linux文件系统初始化时，只是构造了启动时的“根文件系统”，剩下的文件系统一切可以使用系统调用创建出来  

linux的shell是把用户命令翻译成系统调用的序列

文件系统实现即
在一个 I/O 设备 (驱动) 上实现 “目录树” 的数据结构。并且实现相应的API

#### 为什么/proc有一些文件ls和ls -a都看不到，但是可以cd进去

/proc/[tid] 包含了线程号为tid的文件和子目录。目录里的内容和/proc/[pid]/task/[tid]中的一样  
由于/proc/[tid]子目录对getdents系统调用不可见，而ls时使用getdents系统调用来获取/proc的内容的，所以ls看不到


#### FAT文件系统
需求
* 树状的目录结构
* 系统中以小文件为主
* 无需支持链接

实现方式
* 链表

两种设计  
* 在每个数据块后放置指针
  * 优点：实现简单、无须单独开辟存储空间
  * 缺点：数据的大小不是 $2^k$; 单纯的 lseek 需要读整块数据
* 将指针集中存放在文件系统的某个区域
  * 优点：局部性好；lseek 更快
  * 缺点：集中存放的数据损坏将导致数据丢失 

选择后者

集中存储的指针容易损坏，那么存n份，数据丢失的可能更小了。

FAT就是存储next指针的结构

目录是文件或者目录的集合，就是一个key-value mapping (key 是路径名，value 是 “虚拟磁盘” 中的数据)：所以可以用目录文件来表示目录。操作系统在解析时把标记为目录的目录项 “当做” 目录即可


FAT性能
* 小文件很好
* 大文件的随即访问很慢

可靠性
* 维护若干个 FAT 的副本防止元数据损坏
  * 额外的同步开销


#### ext2/UNIX文件系统
按对象方式集中存储文件/目录元数据
* 增强局部性 (更易于缓存)
* 支持链接

为大小文件区分 fast/slow path
* 小的时候应该用数组
* 大的时候应该用树 (B-Tree; Radix-Tree; ...)
  * 支持更快的随机访问

**ext2：磁盘镜像格式**
见slides

ext2的目录文件与 FAT 本质相同：在文件上建立目录的数据结构

## 持久数据的可靠性
RAID：把多块磁盘虚拟化成一块性能更好、更可靠的磁盘

崩溃一致性
* 如何在硬件可
* 
* 
* 能崩溃的情况下保证文件系统 (数据结构) 的一致性

增加持久数据可靠性的方法
* 备份
  * 所有数据同时写入两块磁盘

**RAID**
RAID 的虚拟化是 “反向” 的  
多个->一个

假设我愿意在系统里多接入一块硬盘用于容灾  
* 通过设备驱动程序抽象成 “一个磁盘”  (例如 1TB)
  * 实际上由A,B两块1TB的物理磁盘组成镜像

* readb(V,blk)
  * 可以从A,B中任意一个读取
* writeb(V,blk)
  * 将同样的数据写入A,B的同一位置

RAID (虚拟化) = 虚拟磁盘块到物理磁盘块的 “映射”。

**RAID-0**
多块盘“交替拼接”
* 完全不能容错

**RAID-1**
接入一块盘容错，就是上面的readb和writeb 
* 容忍一块盘fail

**RAID-10**

没时间了，，，先复习把